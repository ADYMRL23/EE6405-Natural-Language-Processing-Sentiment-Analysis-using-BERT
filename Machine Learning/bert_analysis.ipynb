{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1166120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2afeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Dataset\n",
    "training_set = load_dataset('go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450755e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteEntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-691548b0-1fb7ce6e4836f0a4699d54f0;cf0dfc64-d113-4608-8ea3-5e31e134f2c1)\n\nEntry Not Found for url: https://huggingface.co/api/models/google-bert/bert-base-uncased/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:553\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_curlify\u001b[39m(request: requests\u001b[38;5;241m.\u001b[39mPreparedRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a `requests.PreparedRequest` into a curl command (str).\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    Used for debug purposes only.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    Implementation vendored from https://github.com/ofw/curlify/blob/master/curlify.py.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    MIT License Copyright (c) 2016 Egor.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     parts: List[Tuple[Any, Any]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-X\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod),\n\u001b[0;32m    563\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'https://huggingface.co/api/models/google-bert/bert-base-uncased/tree/main/additional_chat_templates?recursive=false&expand=false'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRemoteEntryNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initializing the tokenizer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_of_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(map_of_models[model_name], num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     12\u001b[0m cce \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1159\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2038\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2034\u001b[0m             vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2035\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2036\u001b[0m             )\n\u001b[0;32m   2037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2038\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2044\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2045\u001b[0m         template \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2046\u001b[0m         vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\transformers\\utils\\hub.py:167\u001b[0m, in \u001b[0;36mlist_repo_templates\u001b[1;34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[0;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m    171\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    172\u001b[0m                 path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR,\n\u001b[0;32m    173\u001b[0m                 recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m                 token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    175\u001b[0m             )\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m         ]\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\transformers\\utils\\hub.py:167\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[0;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m    171\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    172\u001b[0m                 path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR,\n\u001b[0;32m    173\u001b[0m                 recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m                 token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    175\u001b[0m             )\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m         ]\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\huggingface_hub\\hf_api.py:3082\u001b[0m, in \u001b[0;36mlist_repo_tree\u001b[1;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[0;32m   3062\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m   3063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_repo_tree\u001b[39m(\n\u001b[0;32m   3064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3072\u001b[0m     token: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3073\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Union[RepoFile, RepoFolder]]:\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3075\u001b[0m \u001b[38;5;124;03m    List a repo tree's files and folders and get information about them.\u001b[39;00m\n\u001b[0;32m   3076\u001b[0m \n\u001b[0;32m   3077\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   3078\u001b[0m \u001b[38;5;124;03m        repo_id (`str`):\u001b[39;00m\n\u001b[0;32m   3079\u001b[0m \u001b[38;5;124;03m            A namespace (user or an organization) and a repo name separated by a `/`.\u001b[39;00m\n\u001b[0;32m   3080\u001b[0m \u001b[38;5;124;03m        path_in_repo (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m \u001b[38;5;124;03m            Relative path of the tree (folder) in the repo, for example:\u001b[39;00m\n\u001b[1;32m-> 3082\u001b[0m \u001b[38;5;124;03m            `\"checkpoints/1fec34a/results\"`. Will default to the root tree (folder) of the repository.\u001b[39;00m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;124;03m        recursive (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;124;03m            Whether to list tree's files and folders recursively.\u001b[39;00m\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;124;03m        expand (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   3086\u001b[0m \u001b[38;5;124;03m            Whether to fetch more information about the tree's files and folders (e.g. last commit and files' security scan results). This\u001b[39;00m\n\u001b[0;32m   3087\u001b[0m \u001b[38;5;124;03m            operation is more expensive for the server so only 50 results are returned per page (instead of 1000).\u001b[39;00m\n\u001b[0;32m   3088\u001b[0m \u001b[38;5;124;03m            As pagination is implemented in `huggingface_hub`, this is transparent for you except for the time it\u001b[39;00m\n\u001b[0;32m   3089\u001b[0m \u001b[38;5;124;03m            takes to get the results.\u001b[39;00m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m   3091\u001b[0m \u001b[38;5;124;03m            The revision of the repository from which to get the tree. Defaults to `\"main\"` branch.\u001b[39;00m\n\u001b[0;32m   3092\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m   3093\u001b[0m \u001b[38;5;124;03m            The type of the repository from which to get the tree (`\"model\"`, `\"dataset\"` or `\"space\"`.\u001b[39;00m\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;124;03m            Defaults to `\"model\"`.\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;124;03m        token (`bool` or `str`, *optional*):\u001b[39;00m\n\u001b[0;32m   3096\u001b[0m \u001b[38;5;124;03m            A valid user access token (string). Defaults to the locally saved\u001b[39;00m\n\u001b[0;32m   3097\u001b[0m \u001b[38;5;124;03m            token, which is the recommended method for authentication (see\u001b[39;00m\n\u001b[0;32m   3098\u001b[0m \u001b[38;5;124;03m            https://huggingface.co/docs/huggingface_hub/quick-start#authentication).\u001b[39;00m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;124;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[0;32m   3100\u001b[0m \n\u001b[0;32m   3101\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;124;03m        `Iterable[Union[RepoFile, RepoFolder]]`:\u001b[39;00m\n\u001b[0;32m   3103\u001b[0m \u001b[38;5;124;03m            The information about the tree's files and folders, as an iterable of [`RepoFile`] and [`RepoFolder`] objects. The order of the files and folders is\u001b[39;00m\n\u001b[0;32m   3104\u001b[0m \u001b[38;5;124;03m            not guaranteed.\u001b[39;00m\n\u001b[0;32m   3105\u001b[0m \n\u001b[0;32m   3106\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m   3107\u001b[0m \u001b[38;5;124;03m        [`~utils.RepositoryNotFoundError`]:\u001b[39;00m\n\u001b[0;32m   3108\u001b[0m \u001b[38;5;124;03m            If repository is not found (error 404): wrong repo_id/repo_type, private but not authenticated or repo\u001b[39;00m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m            does not exist.\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m        [`~utils.RevisionNotFoundError`]:\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;124;03m            If revision is not found (error 404) on the repo.\u001b[39;00m\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;124;03m        [`~utils.EntryNotFoundError`]:\u001b[39;00m\n\u001b[0;32m   3113\u001b[0m \u001b[38;5;124;03m            If the tree (folder) does not exist (error 404) on the repo.\u001b[39;00m\n\u001b[0;32m   3114\u001b[0m \n\u001b[0;32m   3115\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   3116\u001b[0m \n\u001b[0;32m   3117\u001b[0m \u001b[38;5;124;03m        Get information about a repo's tree.\u001b[39;00m\n\u001b[0;32m   3118\u001b[0m \u001b[38;5;124;03m        ```py\u001b[39;00m\n\u001b[0;32m   3119\u001b[0m \u001b[38;5;124;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m        >>> repo_tree = list_repo_tree(\"lysandre/arxiv-nlp\")\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m        >>> repo_tree\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;124;03m        <generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>\u001b[39;00m\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;124;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[0;32m   3124\u001b[0m \u001b[38;5;124;03m        [\u001b[39;00m\n\u001b[0;32m   3125\u001b[0m \u001b[38;5;124;03m            RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[0;32m   3126\u001b[0m \u001b[38;5;124;03m            RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;124;03m            RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;124;03m            RepoFile(\u001b[39;00m\n\u001b[0;32m   3129\u001b[0m \u001b[38;5;124;03m                path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',\u001b[39;00m\n\u001b[0;32m   3130\u001b[0m \u001b[38;5;124;03m                lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[0;32m   3131\u001b[0m \u001b[38;5;124;03m            ),\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;124;03m            RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[0;32m   3133\u001b[0m \u001b[38;5;124;03m            RepoFile(\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[38;5;124;03m                path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',\u001b[39;00m\n\u001b[0;32m   3135\u001b[0m \u001b[38;5;124;03m                lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[0;32m   3136\u001b[0m \u001b[38;5;124;03m            ),\u001b[39;00m\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;124;03m            RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]\u001b[39;00m\n\u001b[0;32m   3138\u001b[0m \u001b[38;5;124;03m        ]\u001b[39;00m\n\u001b[0;32m   3139\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m   3140\u001b[0m \n\u001b[0;32m   3141\u001b[0m \u001b[38;5;124;03m        Get even more information about a repo's tree (last commit and files' security scan results)\u001b[39;00m\n\u001b[0;32m   3142\u001b[0m \u001b[38;5;124;03m        ```py\u001b[39;00m\n\u001b[0;32m   3143\u001b[0m \u001b[38;5;124;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[0;32m   3144\u001b[0m \u001b[38;5;124;03m        >>> repo_tree = list_repo_tree(\"prompthero/openjourney-v4\", expand=True)\u001b[39;00m\n\u001b[0;32m   3145\u001b[0m \u001b[38;5;124;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;124;03m        [\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;124;03m            RepoFolder(\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;124;03m                path='feature_extractor',\u001b[39;00m\n\u001b[0;32m   3149\u001b[0m \u001b[38;5;124;03m                tree_id='aa536c4ea18073388b5b0bc791057a7296a00398',\u001b[39;00m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;124;03m                last_commit={\u001b[39;00m\n\u001b[0;32m   3151\u001b[0m \u001b[38;5;124;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;124;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;124;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[0;32m   3154\u001b[0m \u001b[38;5;124;03m                }\u001b[39;00m\n\u001b[0;32m   3155\u001b[0m \u001b[38;5;124;03m            ),\u001b[39;00m\n\u001b[0;32m   3156\u001b[0m \u001b[38;5;124;03m            RepoFolder(\u001b[39;00m\n\u001b[0;32m   3157\u001b[0m \u001b[38;5;124;03m                path='safety_checker',\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m \u001b[38;5;124;03m                tree_id='65aef9d787e5557373fdf714d6c34d4fcdd70440',\u001b[39;00m\n\u001b[0;32m   3159\u001b[0m \u001b[38;5;124;03m                last_commit={\u001b[39;00m\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;124;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[0;32m   3161\u001b[0m \u001b[38;5;124;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;124;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[0;32m   3163\u001b[0m \u001b[38;5;124;03m                }\u001b[39;00m\n\u001b[0;32m   3164\u001b[0m \u001b[38;5;124;03m            ),\u001b[39;00m\n\u001b[0;32m   3165\u001b[0m \u001b[38;5;124;03m            RepoFile(\u001b[39;00m\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;124;03m                path='model_index.json',\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m \u001b[38;5;124;03m                size=582,\u001b[39;00m\n\u001b[0;32m   3168\u001b[0m \u001b[38;5;124;03m                blob_id='d3d7c1e8c3e78eeb1640b8e2041ee256e24c9ee1',\u001b[39;00m\n\u001b[0;32m   3169\u001b[0m \u001b[38;5;124;03m                lfs=None,\u001b[39;00m\n\u001b[0;32m   3170\u001b[0m \u001b[38;5;124;03m                last_commit={\u001b[39;00m\n\u001b[0;32m   3171\u001b[0m \u001b[38;5;124;03m                    'oid': 'b195ed2d503f3eb29637050a886d77bd81d35f0e',\u001b[39;00m\n\u001b[0;32m   3172\u001b[0m \u001b[38;5;124;03m                    'title': 'Fix deprecation warning by changing `CLIPFeatureExtractor` to `CLIPImageProcessor`. (#54)',\u001b[39;00m\n\u001b[0;32m   3173\u001b[0m \u001b[38;5;124;03m                    'date': datetime.datetime(2023, 5, 15, 21, 41, 59, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;124;03m                },\u001b[39;00m\n\u001b[0;32m   3175\u001b[0m \u001b[38;5;124;03m                security={\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m \u001b[38;5;124;03m                    'safe': True,\u001b[39;00m\n\u001b[0;32m   3177\u001b[0m \u001b[38;5;124;03m                    'av_scan': {'virusFound': False, 'virusNames': None},\u001b[39;00m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;124;03m                    'pickle_import_scan': None\u001b[39;00m\n\u001b[0;32m   3179\u001b[0m \u001b[38;5;124;03m                }\u001b[39;00m\n\u001b[0;32m   3180\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m   3181\u001b[0m \u001b[38;5;124;03m            ...\u001b[39;00m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;124;03m        ]\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m     repo_type \u001b[38;5;241m=\u001b[39m repo_type \u001b[38;5;129;01mor\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mREPO_TYPE_MODEL\n\u001b[0;32m   3186\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(revision, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mDEFAULT_REVISION\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:37\u001b[0m, in \u001b[0;36mpaginate\u001b[1;34m(path, params, headers)\u001b[0m\n\u001b[0;32m     35\u001b[0m session \u001b[38;5;241m=\u001b[39m get_session()\n\u001b[0;32m     36\u001b[0m r \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(path, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Follow pages\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Next link already contains query params\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\Github Projects\\EE6405-Natural-Language-Processing-Sentiment-Analysis-using-BERT\\NLPBERTENV\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:567\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 567\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<TOKEN>\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Hide authorization header, no matter its value (can be Bearer, Key, etc.)\u001b[39;00m\n\u001b[0;32m    568\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-H\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v))]\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mbody:\n",
      "\u001b[1;31mRemoteEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-691548b0-1fb7ce6e4836f0a4699d54f0;cf0dfc64-d113-4608-8ea3-5e31e134f2c1)\n\nEntry Not Found for url: https://huggingface.co/api/models/google-bert/bert-base-uncased/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\""
     ]
    }
   ],
   "source": [
    "# Initializing the BERTS\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Initializing a list of Models\n",
    "map_of_models = {\"BERT\": \"bert-base-uncased\"}\n",
    "\n",
    "# Initializing the tokenizer\n",
    "model_name = \"BERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(map_of_models[model_name])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(map_of_models[model_name], num_labels = 28)\n",
    "cce = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Loop to tokenize the data\n",
    "def tokenize(dataset):\n",
    "    return tokenizer(dataset['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Processing the labels\n",
    "def return_one_label(dataset):\n",
    "    dataset['labels'] = dataset['labels'][0]\n",
    "    return dataset # Only returning the first label of the traub\n",
    "\n",
    "tokenized_dataset = training_set.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.map(return_one_label)\n",
    "\n",
    "# # Function to compute the metrics\n",
    "# def compute_metrics(y_pred):\n",
    "#     logits, labels = y_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return cce.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= f\"{model_name}_trained_weights\",\n",
    "    per_device_train_batch_size = 8192, # 20% of the dataset\n",
    "    per_device_eval_batch_size = 1024, # 20% of the validation set\n",
    "    num_train_epochs = 1,\n",
    "    push_to_hub = False\n",
    ")\n",
    "\n",
    "# Training the Model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset= tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['validation']\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b17bfa",
   "metadata": {},
   "source": [
    "# Just a table to list down all of the results from the training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7be25",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://huggingface.co/docs/transformers/en/training\n",
    "2. https://medium.com/@rajratangulab.more/fine-tuning-bert-for-text-classification-using-hugging-face-transformers-685c132d185d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3485e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 43410\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5427\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbacee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive\\github projects\\ee6405-natural-language-processing-sentiment-analysis-using-bert\\nlpbertenv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.1.2\n",
      "    Uninstalling huggingface_hub-1.1.2:\n",
      "      Successfully uninstalled huggingface_hub-1.1.2\n",
      "Successfully installed huggingface-hub-0.36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17c7e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('imdb')['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPBERTENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
