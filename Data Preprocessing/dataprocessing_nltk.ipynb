{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6d444f",
   "metadata": {},
   "source": [
    "# Data Preprocessing of GoEmotions Dataset by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46da831",
   "metadata": {},
   "source": [
    " There are 3 main reasons why we conduct Data Processing:\n",
    "\n",
    " (a) Remove symbols and irrelevant content\n",
    " \n",
    " (b) Normalise texts to reduce vocabulary size\n",
    "\n",
    " (c) Improve tokenisation\n",
    "\n",
    " In this notebook, GoEmotions Dataset by Google in Kaggle is used. Data cleaning techniques such as RegEx, and stopword removal are used to clean the dataset, before doing tokenization. After that, stemming/lemmatization can be selected as options to reduce vocab size for easier processing. Parts of Speech tagging is used to label the tokens. TF-IDF, NMF and BM25 are then used as part of analysis of the processed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900bc326",
   "metadata": {},
   "source": [
    "Before starting any project that uses datasets, it is generally a good idea to know and understand the data that is being handled. Data in various datasets can be messy and require some cleaning for tokenizers to work. For this project, although we are using BERT and its variants and XLNET which are all pretrained trasnformer models that can handle messy raw text via their respective tokenizers from huggingface, it is a good habit to know and understand the data that is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b280e0d",
   "metadata": {},
   "source": [
    "## 1. Install the Natural Language Toolkit Preprocessing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27b0006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('d:/EE6405 NLP Project/Data Preprocessing/dataset/go_emotions_dataset_shivamb')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # NLTK pre-trained tokenizer model\n",
    "nltk.download('wordnet', quiet=True) # Lexical database for grouping words into sets of synonyms, used for lemmatization\n",
    "nltk.download('omw-1.4', quiet=True) # WordNet but adds multilingual grouping\n",
    "nltk.download('stopwords', quiet=True) # List of multilingual stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "DATA_DIR = Path(r'd:\\EE6405 NLP Project\\Data Preprocessing\\dataset')\n",
    "files = [DATA_DIR / 'go_emotions_dataset_shivamb']\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb1d5",
   "metadata": {},
   "source": [
    "## 2. Load selected CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a58a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file(s) scheduled\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def find_text_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    for cand in ['text','sentence','content','utterance','lyrics','statement']:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    obj_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    return obj_cols[0] if obj_cols else None\n",
    "\n",
    "def load_one(f: Path) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(f)\n",
    "    text_col = find_text_col(df)\n",
    "    if text_col is None:\n",
    "        return None\n",
    "    df = df.rename(columns={text_col: 'text'})\n",
    "    df['source_file'] = f.name\n",
    "    return df\n",
    "\n",
    "#files = list(DATA_DIR.glob('*.csv'))\n",
    "#files\n",
    "print(f\"{len(files)} file(s) scheduled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c90ca7",
   "metadata": {},
   "source": [
    "## 3. Regex Cleaning Function\n",
    "\n",
    "#### Cleaning is to remove noise & normalize text before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e9d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning is to remove noise & normalize text before tokenization\n",
    "\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+') # Removes URLs like http:// or https:// or www.\n",
    "MENTION_PATTERN = re.compile(r'@[A-Za-z0-9_]+') # Removes Twitter mentions like @username\n",
    "HASHTAG_PATTERN = re.compile(r'#(\\w+)') # Keeps hashtagged words but removes the #\n",
    "NON_ALPHA_PATTERN = re.compile(r'[^a-zA-Z\\s]') # Removes non-alphabetic characters except spaces\n",
    "MULTISPACE_PATTERN = re.compile(r'\\s+') # Replaces multiple spaces with a single space\n",
    "def basic_clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = URL_PATTERN.sub(' ', text)\n",
    "    text = MENTION_PATTERN.sub(' ', text)\n",
    "    text = HASHTAG_PATTERN.sub(r'\\1', text)\n",
    "    text = NON_ALPHA_PATTERN.sub(' ', text)\n",
    "    text = MULTISPACE_PATTERN.sub(' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ccf98",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal & Tokenization\n",
    "\n",
    "Stop words are removed to reduce vocabulary size and remove unecessary words.\n",
    "\n",
    "Tokenization is to split sentences into words for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfd7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # NLTK English stop words\n",
    "DOMAIN_STOP = {\"chorus\",\"verse\",\"repeat\",\"na\",\"la\"}  # Add domain specific stop words, change as needed\n",
    "stop_words |= DOMAIN_STOP # Merge both the sets\n",
    "\n",
    "def tokenize_filter(s: str):\n",
    "    tokens = word_tokenize(s) # Tokenize text into words\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2] # Remove stop words and words shorter than 3 characters\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff0b19",
   "metadata": {},
   "source": [
    "## 5. Stemming vs Lemmatization\n",
    "\n",
    "Stemming reduces words to its root/stem (eg studies --> studi).\n",
    "\n",
    "Lemmatization reduces words to their dictionary base form (eg studies --> study).\n",
    "\n",
    "These 2 processes help to reduce words to base forms to reduce vocabulary size for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d7e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # NLTK Word Stemmer\n",
    "lemmatizer = WordNetLemmatizer() # NLTK Word Lemmatizer\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb9cc9",
   "metadata": {},
   "source": [
    "## 6. Choose one representation (Lemmatized/Stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68811bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['clean_text']   = df['text'].apply(basic_clean)\n",
    "    df['tokens']       = df['clean_text'].apply(tokenize_filter)\n",
    "    df['stemmed']      = df['tokens'].apply(stem_tokens)\n",
    "    df['lemmatized']   = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Change to df['stemmed'] to use stemmed version\n",
    "    df['final_text']   = df['lemmatized'].apply(lambda toks: ' '.join(toks)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992799f4",
   "metadata": {},
   "source": [
    "## 7. Part-of-speech (POS) Tagging\n",
    "\n",
    "POS tagging is to label each word (eg. noun, verb, adjective) with reference to its context.\n",
    "\n",
    "Models need this tag to understand sentence structure & semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d623ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure POS tagger models are available\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "def add_pos_tags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add POS tags for each token list in df['tokens'].\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'tokens' not in df.columns:\n",
    "        # Fallback: build tokens from clean_text if needed\n",
    "        df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "\n",
    "    # PTB tags (e.g., NN, VBZ) and Universal tags (e.g., NOUN, VERB)\n",
    "    df['pos_ptb'] = df['tokens'].apply(lambda toks: nltk.pos_tag(toks))\n",
    "    df['pos_universal'] = df['tokens'].apply(lambda toks: nltk.pos_tag(toks, tagset='universal'))\n",
    "\n",
    "    # Counts of Universal POS per doc (useful features)\n",
    "    df['pos_universal_counts'] = df['pos_universal'].apply(lambda pairs: Counter(tag for _, tag in pairs))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af42a3",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF score is an indicator to show how important a word is in relation to its document. High frequency in a document but low frequency in a corpus means high importance as only that document contains that theme. High frequency in both a document and corpus means lower importance as many documents share the same theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e71c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (211225, 253874)\n",
      "\n",
      "Top 10 terms (by max TF-IDF across docs):\n",
      "police -> 1.0000\n",
      "special -> 1.0000\n",
      "reasonable -> 1.0000\n",
      "happen -> 1.0000\n",
      "hallelujah -> 1.0000\n",
      "poisonous -> 1.0000\n",
      "hamberders -> 1.0000\n",
      "specifically -> 1.0000\n",
      "handsome -> 1.0000\n",
      "calm -> 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def _resolve_df_with_final_text():\n",
    "    glb = globals()\n",
    "    if 'df_proc' in glb and isinstance(glb['df_proc'], pd.DataFrame) and 'final_text' in glb['df_proc'].columns:\n",
    "        return glb['df_proc']\n",
    "    if 'df' in glb and isinstance(glb['df'], pd.DataFrame) and 'final_text' in glb['df'].columns:\n",
    "        return glb['df']\n",
    "    try:\n",
    "        srcs = files\n",
    "    except NameError:\n",
    "        raise ValueError(\"Provide a DataFrame (df_proc/df) with 'final_text' or define 'files'.\")\n",
    "    csvs = []\n",
    "    for f in srcs:\n",
    "        p = Path(f)\n",
    "        if p.is_dir():\n",
    "            csvs.extend(sorted(p.glob('*.csv')))\n",
    "            continue\n",
    "        if p.suffix.lower() != '.csv':\n",
    "            cand = p.with_suffix('.csv')\n",
    "            if cand.exists():\n",
    "                p = cand\n",
    "        if p.exists() and p.is_file() and p.suffix.lower() == '.csv':\n",
    "            csvs.append(p)\n",
    "    if not csvs:\n",
    "        raise ValueError(\"No CSVs resolved from 'files'.\")\n",
    "    dfs = []\n",
    "    for f in csvs:\n",
    "        d = load_one(Path(f))\n",
    "        if d is not None:\n",
    "            dfs.append(preprocess_df(d))\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No valid dataframes built. Check text column detection.\")\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Build corpus\n",
    "df_nmf = _resolve_df_with_final_text()\n",
    "corpus = df_nmf['final_text'].fillna('')\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False  # already lowercased in cleaning\n",
    ")\n",
    "X = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "\n",
    "import numpy as np\n",
    "feat = tfidf.get_feature_names_out()\n",
    "max_scores = X.max(axis=0).toarray().ravel()\n",
    "top_idx = np.argsort(max_scores)[::-1][:10]\n",
    "print(\"\\nTop 10 terms (by max TF-IDF across docs):\")\n",
    "for i in top_idx:\n",
    "    print(f\"{feat[i]} -> {max_scores[i]:.4f}\")\n",
    "\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53532f3",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "After getting the TF-IDF matrix, we factorise it into a document-topic matrix and topic-word matrix. This is done to find latent topics within a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3e2b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: name, name name, like name, love name, thank name, name love, name would, damn, hate, miss name, think name, miss\n",
      "Topic 1: thank, thank name, thank much, thank sharing, appreciate, thank service, sharing, awesome thank, advice, thank thank, name thank, service\n",
      "Topic 2: like, like name, sound, sound like, look like, feel like, really like, seems like, seems, like one, name like, something\n",
      "Topic 3: love, love name, name love, would love, love username, username, love see, really love, love guy, thanks love, love love, got love\n",
      "Topic 4: thanks, thanks name, thanks man, thanks sharing, sharing, thanks hate, thanks much, thanks info, info, cool, appreciate, advice\n",
      "Topic 5: good, luck, good luck, good one, good job, job, good know, really good, name good, good thing, good name, good idea\n",
      "Topic 6: think, people, mean, think name, way, even, need, thing, still, wrong, hate, lot\n",
      "Topic 7: happy, day, cake, cake day, happy cake, birthday, happy birthday, happy new, new year, make happy, remindme day, remindme\n",
      "Topic 8: one, good one, like one, favorite, one favorite, one day, thought, ever, name one, best, day, one name\n",
      "Topic 9: sorry, sorry loss, loss, sorry hear, hear, happened, sorry happened, hope, sorry name, say, name sorry, sorry sorry\n",
      "Topic 10: lol, lol name, actually, name lol, lol love, lol good, well, funny, lol thanks, made, said, like lol\n",
      "Topic 11: get, hope, better, get better, name get, help, back, hope get, need, get back, get help, let\n",
      "Topic 12: look, look like, name look, pretty, like name, cool, look good, guy, look pretty, amazing, make look, look name\n",
      "Topic 13: know, know name, good know, even, well, want, even know, thing, let, guy, dont, say\n",
      "Topic 14: yes, yes yes, please, yes name, name yes, hell, yes please, hell yes, say, omg yes, omg, yes thank\n",
      "Topic 15: bad, feel bad, really bad, thing, bad name, bad thing, look bad, guy, even, name bad, game, good bad\n",
      "Topic 16: really, really like, hope, wow, really good, name really, cool, actually, want, interesting, really hope, really love\n",
      "Topic 17: see, see name, want, glad, want see, love see, nice, wait, glad see, wait see, let, well\n",
      "Topic 18: year, old, new, new year, year old, happy new, ago, year ago, last, happy, still, last year\n",
      "Topic 19: feel, feel like, better, way, feel better, feel bad, make feel, hope, made, feel good, made feel, feel way\n",
      "Topic 20: would, say, name would, would love, never, thought, cool, wish, nice, awesome, would like, someone\n",
      "Topic 21: got, name got, damn, glad, wow, guy, got name, got love, glad got, got get, sure, got good\n",
      "Topic 22: much, thank much, better, hate, much better, pretty, pretty much, name much, thanks much, love much, wow, way\n",
      "Topic 23: right, damn, name right, wrong, hope, thing, right name, know right, alt right, alt, left, say\n",
      "Topic 24: time, every, first, long, first time, every time, next, take, next time, long time, work, many\n",
      "Topic 25: yeah, hell, hell yeah, well, mean, boy yeah, boy, dude, yeah know, haha, yeah yeah, weird\n",
      "Topic 26: great, wow, work, idea, name great, amazing, awesome, game, great idea, sound, hope, great name\n",
      "Topic 27: make, sense, make sense, want, sure, thing, even, make feel, make sure, make happy, name make, better\n",
      "Topic 28: fuck, fuck name, guy, fucking, say, actual fuck, actual, holy, holy fuck, even, shit, well\n",
      "Topic 29: man, thanks man, good man, man name, well, name man, awesome, old man, nice, old, man love, suck\n",
      "NMF done. Doc-topic shape: (211225, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import joblib\n",
    "\n",
    "# Ensure prerequisites exist\n",
    "assert 'X' in globals() and 'tfidf' in globals() and 'df_nmf' in globals(), \"Run the TF-IDF cell first.\"\n",
    "\n",
    "# NMF\n",
    "n_topics = 30  # Split into 30 topics\n",
    "nmf = NMF(\n",
    "    n_components=n_topics,\n",
    "    init='nndsvd',\n",
    "    random_state=42,\n",
    "    max_iter=400,\n",
    "    alpha_W=0.0,\n",
    "    l1_ratio=0.0\n",
    ")\n",
    "W = nmf.fit_transform(X)  # doc-topic matrix\n",
    "H = nmf.components_      # topic-term matrix\n",
    "feat = tfidf.get_feature_names_out()\n",
    "\n",
    "# Show top terms per topic\n",
    "topn = 12\n",
    "for k, comp in enumerate(H):\n",
    "    top_idx = np.argsort(comp)[::-1][:topn]\n",
    "    terms = [feat[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(terms)}\")\n",
    "\n",
    "# Attach dominant topic to dataframe\n",
    "df_nmf['nmf_topic'] = W.argmax(axis=1)\n",
    "df_nmf['nmf_strength'] = W.max(axis=1)\n",
    "\n",
    "# Persist artifacts\n",
    "joblib.dump(nmf, 'nmf_model.joblib')\n",
    "joblib.dump(W, 'nmf_doc_topic.joblib')\n",
    "joblib.dump(H, 'nmf_topic_term.joblib')\n",
    "df_nmf.to_csv('df_with_nmf.csv', index=False)\n",
    "\n",
    "print(\"NMF done. Doc-topic shape:\", W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f3462",
   "metadata": {},
   "source": [
    "## Best Match 25 (BM25)\n",
    "\n",
    "BM25 is an algorithm that considers both term frequency (TF) and document length normalisation to determine the relevance of a document to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d59a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_text' is the post-processed text used for BM25 search.\n",
      "'text' is the original pre-processed text.\n",
      "\n",
      "        bm25_score                                 final_text  \\\n",
      "16657     8.963676  interesting fresh water dilute salt water   \n",
      "55744     8.963676  interesting fresh water dilute salt water   \n",
      "185409    8.963676  interesting fresh water dilute salt water   \n",
      "165011    8.963676  interesting fresh water dilute salt water   \n",
      "102250    8.963676  interesting fresh water dilute salt water   \n",
      "134479    8.911442                                spicy water   \n",
      "155168    8.911442                               filled water   \n",
      "87151     8.911442                                spicy water   \n",
      "75331     8.911442                               filled water   \n",
      "144509    8.911442                                spicy water   \n",
      "\n",
      "                                                     text  \\\n",
      "16657   It's very interesting that the fresh water doe...   \n",
      "55744   It's very interesting that the fresh water doe...   \n",
      "185409  It's very interesting that the fresh water doe...   \n",
      "165011  It's very interesting that the fresh water doe...   \n",
      "102250  It's very interesting that the fresh water doe...   \n",
      "134479                                       Spicy water.   \n",
      "155168                                 Filled with water?   \n",
      "87151                                        Spicy water.   \n",
      "75331                                  Filled with water?   \n",
      "144509                                       Spicy water.   \n",
      "\n",
      "                            source_file  \n",
      "16657   go_emotions_dataset_shivamb.csv  \n",
      "55744   go_emotions_dataset_shivamb.csv  \n",
      "185409  go_emotions_dataset_shivamb.csv  \n",
      "165011  go_emotions_dataset_shivamb.csv  \n",
      "102250  go_emotions_dataset_shivamb.csv  \n",
      "134479  go_emotions_dataset_shivamb.csv  \n",
      "155168  go_emotions_dataset_shivamb.csv  \n",
      "87151   go_emotions_dataset_shivamb.csv  \n",
      "75331   go_emotions_dataset_shivamb.csv  \n",
      "144509  go_emotions_dataset_shivamb.csv  \n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25\"])\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def _df_for_bm25():\n",
    "    glb = globals()\n",
    "    if 'df_proc' in glb and isinstance(glb['df_proc'], pd.DataFrame):\n",
    "        return glb['df_proc']\n",
    "    try:\n",
    "        return _resolve_df_with_final_text()\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"No dataframe available. Run preprocessing first.\") from e\n",
    "\n",
    "def _docs_tokens(df_base: pd.DataFrame):\n",
    "    if 'final_text' in df_base.columns:\n",
    "        return df_base['final_text'].fillna('').str.split().tolist()\n",
    "    if 'tokens' in df_base.columns:\n",
    "        return df_base['tokens'].tolist()\n",
    "    if 'clean_text' in df_base.columns:\n",
    "        return df_base['clean_text'].fillna('').apply(word_tokenize).tolist()\n",
    "    return df_base['text'].fillna('').apply(lambda s: word_tokenize(basic_clean(s))).tolist()\n",
    "\n",
    "def _prep_query_tokens(q: str):\n",
    "    q_clean = basic_clean(q)\n",
    "    toks = tokenize_filter(q_clean)\n",
    "    return lemmatize_tokens(toks)\n",
    "\n",
    "# Build index once\n",
    "df_bm = _df_for_bm25()\n",
    "docs_tokens = _docs_tokens(df_bm)\n",
    "bm25 = BM25Okapi(docs_tokens, k1=1.5, b=0.75)\n",
    "\n",
    "def bm25_search_rank(df_base: pd.DataFrame, query: str, top_k=10):\n",
    "    q_toks = _prep_query_tokens(query)\n",
    "    scores = bm25.get_scores(q_toks)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    out = df_base.iloc[top_idx].copy()\n",
    "    out['bm25_score'] = np.array(scores)[top_idx]\n",
    "    cols = [c for c in ['bm25_score', 'final_text', 'text', 'source_file'] if c in out.columns]\n",
    "    return out[cols]\n",
    "\n",
    "print(\"'final_text' is the post-processed text used for BM25 search.\\n'text' is the original pre-processed text.\\n\")\n",
    "# Input word query here\n",
    "results = bm25_search_rank(df_bm, \"water\", top_k=10)\n",
    "print(results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
