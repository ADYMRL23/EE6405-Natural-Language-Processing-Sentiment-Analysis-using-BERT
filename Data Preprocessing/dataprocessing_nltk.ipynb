{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b280e0d",
   "metadata": {},
   "source": [
    "## 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27b0006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('d:/EE6405 NLP Project/Data Preprocessing/dataset/song_lyrics_CarlosGDCJ_en_filtered_en_1M')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "DATA_DIR = Path(r'd:\\EE6405 NLP Project\\Data Preprocessing\\dataset')\n",
    "files = [DATA_DIR / 'song_lyrics_CarlosGDCJ_en_filtered_en_1M']\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb1d5",
   "metadata": {},
   "source": [
    "## 2. Load selected CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a58a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file(s) scheduled\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def find_text_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    for cand in ['text','sentence','content','utterance','lyrics','statement']:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    obj_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    return obj_cols[0] if obj_cols else None\n",
    "\n",
    "def load_one(f: Path) -> Optional[pd.DataFrame]:\n",
    "    df = pd.read_csv(f)\n",
    "    text_col = find_text_col(df)\n",
    "    if text_col is None:\n",
    "        return None\n",
    "    df = df.rename(columns={text_col: 'text'})\n",
    "    df['source_file'] = f.name\n",
    "    return df\n",
    "\n",
    "# Keep files list from Cell 1\n",
    "#files = list(DATA_DIR.glob('*.csv'))\n",
    "#files\n",
    "print(f\"{len(files)} file(s) scheduled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c90ca7",
   "metadata": {},
   "source": [
    "## 3. Regex cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e9d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_PATTERN = re.compile(r'@[A-Za-z0-9_]+')\n",
    "HASHTAG_PATTERN = re.compile(r'#(\\w+)')\n",
    "NON_ALPHA_PATTERN = re.compile(r'[^a-zA-Z\\s]')\n",
    "MULTISPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = URL_PATTERN.sub(' ', text)\n",
    "    text = MENTION_PATTERN.sub(' ', text)\n",
    "    text = HASHTAG_PATTERN.sub(r'\\1', text)  # keep hashtag word\n",
    "    text = NON_ALPHA_PATTERN.sub(' ', text)\n",
    "    text = MULTISPACE_PATTERN.sub(' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ccf98",
   "metadata": {},
   "source": [
    "## 4. Tokenization, Custom Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfd7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "DOMAIN_STOP = {\"chorus\",\"verse\",\"repeat\",\"na\",\"la\"}  # extend for lyrics\n",
    "stop_words |= DOMAIN_STOP\n",
    "\n",
    "def tokenize_filter(s: str):\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff0b19",
   "metadata": {},
   "source": [
    "## 5. Stemming vs Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d7e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb9cc9",
   "metadata": {},
   "source": [
    "## 6. Choose one representation (Lemmatized/Stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68811bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['clean_text']   = df['text'].apply(basic_clean)\n",
    "    df['tokens']       = df['clean_text'].apply(tokenize_filter)\n",
    "    df['stemmed']      = df['tokens'].apply(stem_tokens)\n",
    "    df['lemmatized']   = df['tokens'].apply(lemmatize_tokens)\n",
    "    # Choose representation (lemmatized by default)\n",
    "    df['final_text']   = df['lemmatized'].apply(lambda toks: ' '.join(toks))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992799f4",
   "metadata": {},
   "source": [
    "## 7. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d623ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure POS tagger models are available\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "def add_pos_tags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add POS tags for each token list in df['tokens'].\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'tokens' not in df.columns:\n",
    "        # Fallback: build tokens from clean_text if needed\n",
    "        df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "\n",
    "    # PTB tags (e.g., NN, VBZ) and Universal tags (e.g., NOUN, VERB)\n",
    "    df['pos_ptb'] = df['tokens'].apply(lambda toks: nltk.pos_tag(toks))\n",
    "    df['pos_universal'] = df['tokens'].apply(lambda toks: nltk.pos_tag(toks, tagset='universal'))\n",
    "\n",
    "    # Counts of Universal POS per doc (useful features)\n",
    "    df['pos_universal_counts'] = df['pos_universal'].apply(lambda pairs: Counter(tag for _, tag in pairs))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af42a3",
   "metadata": {},
   "source": [
    "## TF-IDF & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a82b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: know, never, time, could, thing, say, think, cause, always, want, see, tell\n",
      "Topic 1: nigga, bitch, shit, fuck, got, like, get, money, hook, hit, as, hoe\n",
      "Topic 2: baby, girl, wan, let, want, got, know, come, gon, get, tonight, take\n",
      "Topic 3: man, one, well, little, like, old, said, good, people, two, year, first\n",
      "Topic 4: eye, life, light, world, soul, see, sky, god, heart, fire, death, blood\n",
      "Topic 5: love, love love, heart, know love, true, love like, give, say love, love know, fall love, forever, need love\n",
      "Topic 6: yeah, yeah yeah, yeah know, yeah got, woah, intro, got, hey, outro, know yeah, ayy, huh\n",
      "Topic 7: away, day, home, night, way, come, long, back, gone, far, stay, time\n",
      "Topic 8: feel, feel like, like, make feel, feeling, make, wan, know feel, inside, feel feel, real, feel good\n",
      "Topic 9: ooh, ooh ooh, pre, ooh yeah, yeah ooh, outro, bridge, ooh baby, love ooh, outro ooh, post, oooh\n",
      "NMF done. Doc-topic shape: (1000000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import joblib\n",
    "\n",
    "def _resolve_df_with_final_text():\n",
    "    glb = globals()\n",
    "    # Prefer preprocessed dataframes\n",
    "    if 'df_proc' in glb and isinstance(glb['df_proc'], pd.DataFrame) and 'final_text' in glb['df_proc'].columns:\n",
    "        return glb['df_proc']\n",
    "    if 'df' in glb and isinstance(glb['df'], pd.DataFrame) and 'final_text' in glb['df'].columns:\n",
    "        return glb['df']\n",
    "    # Fallback: load from `files` using your helpers\n",
    "    try:\n",
    "        srcs = files\n",
    "    except NameError:\n",
    "        raise ValueError(\"Provide a DataFrame (df_proc/df) with 'final_text' or define 'files'.\")\n",
    "    csvs = []\n",
    "    for f in srcs:\n",
    "        p = Path(f)\n",
    "        if p.is_dir():\n",
    "            csvs.extend(sorted(p.glob('*.csv')))\n",
    "            continue\n",
    "        if p.suffix.lower() != '.csv':\n",
    "            cand = p.with_suffix('.csv')\n",
    "            if cand.exists():\n",
    "                p = cand\n",
    "        if p.exists() and p.is_file() and p.suffix.lower() == '.csv':\n",
    "            csvs.append(p)\n",
    "    if not csvs:\n",
    "        raise ValueError(\"No CSVs resolved from 'files'.\")\n",
    "    dfs = []\n",
    "    for f in csvs:\n",
    "        d = load_one(Path(f))\n",
    "        if d is not None:\n",
    "            dfs.append(preprocess_df(d))\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No valid dataframes built. Check text column detection.\")\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Get corpus\n",
    "df_nmf = _resolve_df_with_final_text()\n",
    "corpus = df_nmf['final_text'].fillna('')\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False  # already lowercased in cleaning\n",
    ")\n",
    "X = tfidf.fit_transform(corpus)\n",
    "\n",
    "# NMF\n",
    "n_topics = 10  # tune as needed\n",
    "nmf = NMF(\n",
    "    n_components=n_topics,\n",
    "    init='nndsvd',\n",
    "    random_state=42,\n",
    "    max_iter=400,\n",
    "    alpha_W=0.0,\n",
    "    l1_ratio=0.0\n",
    ")\n",
    "W = nmf.fit_transform(X)  # doc-topic matrix\n",
    "H = nmf.components_      # topic-term matrix\n",
    "feat = tfidf.get_feature_names_out()\n",
    "\n",
    "# Show top terms per topic\n",
    "topn = 12\n",
    "for k, comp in enumerate(H):\n",
    "    top_idx = np.argsort(comp)[::-1][:topn]\n",
    "    terms = [feat[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(terms)}\")\n",
    "\n",
    "# Attach dominant topic to dataframe\n",
    "df_nmf['nmf_topic'] = W.argmax(axis=1)\n",
    "df_nmf['nmf_strength'] = W.max(axis=1)\n",
    "\n",
    "# Optional: persist artifacts\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(nmf, 'nmf_model.joblib')\n",
    "joblib.dump(W, 'nmf_doc_topic.joblib')\n",
    "joblib.dump(H, 'nmf_topic_term.joblib')\n",
    "df_nmf.to_csv('df_with_nmf.csv', index=False)\n",
    "\n",
    "print(\"NMF done. Doc-topic shape:\", W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f3462",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d59a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        bm25_score                                         final_text  \\\n",
      "795175   28.339221  fool dream laugh loud dance dance proud knowle...   \n",
      "124056   19.704714  clown jumping laugh scream jump shout audience...   \n",
      "868944   18.447535  hark bell sweet silver bell seem say throw car...   \n",
      "499370   17.943365  come faithful joyful triumphant come come beth...   \n",
      "604873   17.938690  intro let hello woke feeling great forgot desp...   \n",
      "908222   17.784529  know sensitive people feel much mood seems coo...   \n",
      "701438   17.567383  woke smoked grabbed newspaper ignore name sad ...   \n",
      "907977   17.149233  waoh waoh everything seems sad everything like...   \n",
      "869094   16.925925  hark bell sweet silver bell seem say throw car...   \n",
      "691042   16.870400  bring block bell join along let play happy hol...   \n",
      "\n",
      "                                                     text  \\\n",
      "795175  I'm a fool who dreams\\nI laugh to loud\\nAnd wh...   \n",
      "124056  The clowns are jumping in and out\\nThey laugh,...   \n",
      "868944  [Verse]\\nHark! how the bells\\nSweet silver bel...   \n",
      "499370  Oh come all ye faithful\\nJoyful and triumphant...   \n",
      "604873  [Intro]\\nLet's go\\n\\n[Verse 1]\\nHello there!\\n...   \n",
      "908222  I know that I'm so sensitive\\n\\nBut some peopl...   \n",
      "701438  I woke up\\nI smoked\\nI grabbed the newspaper\\n...   \n",
      "907977  Oh waoh waoh why everything seems so sad,why e...   \n",
      "869094  Hark! How the bells, sweet silver bells\\nAll s...   \n",
      "691042  Chorus:\\nBring your blocks and bells and join ...   \n",
      "\n",
      "                                         source_file  \n",
      "795175  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "124056  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "868944  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "499370  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "604873  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "908222  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "701438  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "907977  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "869094  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n",
      "691042  song_lyrics_CarlosGDCJ_en_filtered_en_1M.csv  \n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  # or BM25Plus\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25\"])\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def _df_for_bm25():\n",
    "    glb = globals()\n",
    "    if 'df_proc' in glb and isinstance(glb['df_proc'], pd.DataFrame):\n",
    "        return glb['df_proc']\n",
    "    try:\n",
    "        return _resolve_df_with_final_text()\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"No dataframe available. Run preprocessing first.\") from e\n",
    "\n",
    "def _docs_tokens(df_base: pd.DataFrame):\n",
    "    if 'final_text' in df_base.columns:\n",
    "        return df_base['final_text'].fillna('').str.split().tolist()\n",
    "    if 'tokens' in df_base.columns:\n",
    "        return df_base['tokens'].tolist()\n",
    "    if 'clean_text' in df_base.columns:\n",
    "        return df_base['clean_text'].fillna('').apply(word_tokenize).tolist()\n",
    "    return df_base['text'].fillna('').apply(lambda s: word_tokenize(basic_clean(s))).tolist()\n",
    "\n",
    "def _prep_query_tokens(q: str):\n",
    "    q_clean = basic_clean(q)\n",
    "    toks = tokenize_filter(q_clean)\n",
    "    return lemmatize_tokens(toks)\n",
    "\n",
    "# Build index once\n",
    "df_bm = _df_for_bm25()\n",
    "docs_tokens = _docs_tokens(df_bm)\n",
    "bm25 = BM25Okapi(docs_tokens, k1=1.5, b=0.75)\n",
    "\n",
    "def bm25_search_rank(df_base: pd.DataFrame, query: str, top_k=10):\n",
    "    q_toks = _prep_query_tokens(query)\n",
    "    scores = bm25.get_scores(q_toks)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    out = df_base.iloc[top_idx].copy()\n",
    "    out['bm25_score'] = np.array(scores)[top_idx]\n",
    "    cols = [c for c in ['bm25_score', 'final_text', 'text', 'source_file'] if c in out.columns]\n",
    "    return out[cols]\n",
    "\n",
    "# Example\n",
    "results = bm25_search_rank(df_bm, \"happy joyful excited\", top_k=10)\n",
    "print(results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
